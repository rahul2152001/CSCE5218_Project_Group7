{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdJxEATGnFln",
    "outputId": "06507996-98b2-4e3d-e689-c9e3dd53d6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: spacy 3.8.5\n",
      "Uninstalling spacy-3.8.5:\n",
      "  Successfully uninstalled spacy-3.8.5\n",
      "Found existing installation: thinc 8.3.6\n",
      "Uninstalling thinc-8.3.6:\n",
      "  Successfully uninstalled thinc-8.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y spacy thinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg4LvHgpof06",
    "outputId": "73777220-0ab8-40a6-eccf-1ea5b3d920a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --quiet gensim==4.3.3 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9dwPYrTok9o",
    "outputId": "85f7783c-66db-4c40-8078-e64dbd1f3d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Block 1 complete — DEVICE = cuda\n",
      "  Data root:       /content/drive/MyDrive/VQA_Project/sample_data\n",
      "  Questions JSON:  /content/drive/MyDrive/VQA_Project/sample_data/questions/train2014_questions_subset.json\n",
      "  Annotations JSON:/content/drive/MyDrive/VQA_Project/sample_data/annotations/train2014_annotations_subset.json\n",
      "  Images dir:      /content/drive/MyDrive/VQA_Project/sample_data/images/train2014\n"
     ]
    }
   ],
   "source": [
    "# Imports & Configuration\n",
    "# Importing libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Define paths to dataset components stored in Google Drive\n",
    "DATA_ROOT        = \"/content/drive/MyDrive/VQA_Project/sample_data\"\n",
    "QUESTIONS_JSON   = os.path.join(DATA_ROOT, \"questions\",   \"train2014_questions_subset.json\")\n",
    "ANNOTATIONS_JSON = os.path.join(DATA_ROOT, \"annotations\", \"train2014_annotations_subset.json\")\n",
    "IMAGES_DIR       = os.path.join(DATA_ROOT, \"images\",      \"train2014\")\n",
    "SUBSET_IDS_TXT   = os.path.join(DATA_ROOT, \"subsets\",     \"train_subset_ids.txt\")\n",
    "\n",
    "# Hyperparameters\n",
    "SEED        = 42\n",
    "BATCH_SIZE  = 4\n",
    "NUM_EPOCHS  = 10\n",
    "LR          = 1e-3\n",
    "TOP_ANSWERS = 1000\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "print(f\"  Block 1 complete — DEVICE = {DEVICE}\")\n",
    "print(f\"  Data root:       {DATA_ROOT}\")\n",
    "print(f\"  Questions JSON:  {QUESTIONS_JSON}\")\n",
    "print(f\"  Annotations JSON:{ANNOTATIONS_JSON}\")\n",
    "print(f\"  Images dir:      {IMAGES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BfV3Ntygov3E",
    "outputId": "d29d53cb-3724-4417-e10f-b0afab1a5f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️  Built 15,000 records\n",
      "✔️  Answer vocab size = 1001 (top 1000 + <unk>)\n"
     ]
    }
   ],
   "source": [
    "# Build `records` list & answer vocabulary\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading subset image-ID list\n",
    "with open(SUBSET_IDS_TXT, \"r\") as f:\n",
    "    subset_ids = set(int(line.strip()) for line in f)\n",
    "\n",
    "# Loading questions and annotations\n",
    "with open(QUESTIONS_JSON, \"r\") as f:\n",
    "    questions = json.load(f)[\"questions\"]\n",
    "\n",
    "with open(ANNOTATIONS_JSON, \"r\") as f:\n",
    "    annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "# Map question_id → list of answers (lower-cased)\n",
    "ans_map = {}\n",
    "for ann in annotations:\n",
    "    qid = ann[\"question_id\"]\n",
    "    ans_map.setdefault(qid, []).extend(a[\"answer\"].lower() for a in ann[\"answers\"])\n",
    "\n",
    "# Building a records list\n",
    "records = []\n",
    "for q in questions:\n",
    "    qid, img_id = q[\"question_id\"], q[\"image_id\"]\n",
    "    if img_id not in subset_ids or qid not in ans_map:\n",
    "        continue\n",
    "    img_file = f\"COCO_train2014_{img_id:012d}.jpg\"\n",
    "    img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "    records.append({\n",
    "        \"image_path\": img_path,\n",
    "        \"question\":   q[\"question\"],\n",
    "        \"answers\":    ans_map[qid]\n",
    "    })\n",
    "\n",
    "print(f\"✔️  Built {len(records):,} records\")\n",
    "\n",
    "# Building answer vocabulary i.e. top 1000ans+ <unk>\n",
    "answer_counter = Counter(ans for rec in records for ans in rec[\"answers\"])\n",
    "top_answers    = answer_counter.most_common(TOP_ANSWERS)\n",
    "\n",
    "answer_vocab = {ans: idx for idx, (ans, _) in enumerate(top_answers)}\n",
    "answer_vocab[\"<unk>\"] = len(answer_vocab)\n",
    "\n",
    "print(f\"✔️  Answer vocab size = {len(answer_vocab)} (top {TOP_ANSWERS} + <unk>)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wBAHM2XrHAA",
    "outputId": "5dcf4b13-3769-496c-9796-61c3d1157c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# Configuring Hugging-Face libs and import basics\n",
    "!pip install --quiet transformers==4.39.3 accelerate\n",
    "\n",
    "import torch, random\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "print(\"Torch:\", torch.__version__)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwirjX7IrKCn"
   },
   "outputs": [],
   "source": [
    "# Load the ViLT processor from Hugging Face\n",
    "# The ViltProcessor handles both image and text preprocessing, including tokenization and feature extraction.\n",
    "# 'dandelin/vilt-b32-mlm' is a pre-trained Vision-and-Language Transformer model fine-tuned for masked language modeling.\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "\n",
    "#Total number of answer classes\n",
    "NUM_ANS     = len(answer_vocab)             # e.g. 1 001\n",
    "#reverse map from index to the answer label\n",
    "id2label    = {i: a for a,i in answer_vocab.items()}\n",
    "label2id    = answer_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzQaT1zcrPe5"
   },
   "outputs": [],
   "source": [
    "# ---- constants shared by DataLoader workers ----\n",
    "MAX_Q_LEN = 32        # ViLT can handle up to 40+ but 32 is common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2mvInVBrPbW"
   },
   "outputs": [],
   "source": [
    "# VQA data class with one-hot label vector\n",
    "class ViltVQADataset(Dataset):\n",
    "    def __init__(self, recs):\n",
    "        self.recs = recs\n",
    "        self.resize = transforms.Resize((224,224))\n",
    "    def __len__(self): return len(self.recs)\n",
    "    def __getitem__(self, idx):\n",
    "        rec   = self.recs[idx]\n",
    "        #Load and pre process the image, i.e. convert it inro RGB and resize\n",
    "        img   = self.resize(Image.open(rec[\"image_path\"]).convert(\"RGB\"))\n",
    "\n",
    "        #Using ViLT processor for tokenization\n",
    "        enc = processor(\n",
    "            images=img,\n",
    "            text=rec[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_Q_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "\n",
    "        # Create one-hot encoded target vector for the most frequent answer\n",
    "        gt_ans  = Counter(rec[\"answers\"]).most_common(1)[0][0]\n",
    "        vec     = torch.zeros(NUM_ANS, dtype=torch.float32)\n",
    "        vec[label2id.get(gt_ans, label2id[\"<unk>\"])] = 1.0\n",
    "        enc[\"labels\"] = vec            # shape (1001,)\n",
    "\n",
    "        return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGFHV220rPZS",
    "outputId": "e3315bd3-4658-4b7b-899d-d3f41d5eba76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1594  |  Val batches: 282\n"
     ]
    }
   ],
   "source": [
    "# Dataloader setup for training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_rec, val_rec = train_test_split(records, test_size=0.15, random_state=42) # training=85% and validation=15%\n",
    "\n",
    "BATCH = 8           # fits on Colab T4; reduce if OOM\n",
    "#Initialization of Dataloader for training\n",
    "train_dl = DataLoader(ViltVQADataset(train_rec), batch_size=BATCH,\n",
    "                      shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dl   = DataLoader(ViltVQADataset(val_rec),   batch_size=BATCH,\n",
    "                      shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(f\"Train batches: {len(train_dl)}  |  Val batches: {len(val_dl)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OlU__FErPXO",
    "outputId": "4ccd671e-9474-428b-eb03-02ca0ebf877c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-finetuned-vqa and are newly initialized because the shapes did not match:\n",
      "- classifier.3.weight: found shape torch.Size([3129, 1536]) in the checkpoint and torch.Size([1001, 1536]) in the model instantiated\n",
      "- classifier.3.bias: found shape torch.Size([3129]) in the checkpoint and torch.Size([1001]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-13-0fb0e7ec89d2>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = torch.cuda.amp.GradScaler()      # mixed precision\n"
     ]
    }
   ],
   "source": [
    "# Load ViLT model and set up optimizer\n",
    "model = ViltForQuestionAnswering.from_pretrained(\n",
    "    \"dandelin/vilt-b32-finetuned-vqa\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "# setup AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scaler    = torch.cuda.amp.GradScaler()      # mixed precision\n",
    "EPOCHS    = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362,
     "referenced_widgets": [
      "9776d06760584b37b8ef61ff1dfc21e4",
      "01c80e66326343dc9bd626f710ca2123",
      "b7f9b7ef4539464796561e94b4246a36",
      "401bbd8cc5b64d2ba928ad29308a9c00",
      "497972c332e74e9b946cadb829ce9444",
      "7ce799b646ad4bf7903b8f534af6f794",
      "ee72d33e198c4068901f4e34fe183b34",
      "7b6a7a8a0b394842a3062bdfcdd20b68",
      "f6554467087840cf80bc531fb75af630",
      "819aa59361a046ff8380a5b1597212e0",
      "3a15c08d808d46b9b4ddfcd5c5c73149",
      "8230e3abc9d84534aa514386868743ab",
      "c80182e9de8744caab847e7a97ccc197",
      "736b216408a445c795721751492bb806",
      "d6bc4aac531d49ea85c304daa888590c",
      "e6601e52ab81463fa221df036a50aed6",
      "c780fd4c686f42ceadfd8fa51406cebc",
      "2c80e7646a3244c3b7344510cdd57a82",
      "e4f2d183721940dab141ce7c3f147b13",
      "6de1fa0c99e0406d8acf24729f56f083",
      "0a9eab3c815841afa38b1c9f5ee2cd43",
      "d98c31c1ec004748882dec557f68de43",
      "a49e9793b2e64a3caaf2e6f5e9e24a48",
      "3ea922d39df045d19edb5b78d370c57b",
      "c7af872f205e4954ac9b27c48a82e0fa",
      "a9a4efdb08bd4d8599653c2a0b0c39de",
      "903964213aed4adc9cf819c0c7b56bd5",
      "d96db02f760b40f2912b9c764cb4287a",
      "67c13294da4e4a36988bf15990c3c3e1",
      "a7e12e2301a04a899c8c90e022f3ec3b",
      "0c9898b256064ce0a1cab9b9b43c7eaa",
      "c3aa14effeb8461bb0ef17e166a4b858",
      "a437b24b3ce545da804a8ce199a6ef48",
      "190376ba45a047939802c5847e1a66a5",
      "d281215b48144dffb5f733003453cfa4",
      "15d5d4cd231b446799aec76c93cfbdbd",
      "efd14009c2c946d8b28ffda009bc5388",
      "f7b5c3d3d8f3464990508bd966ac95c4",
      "6de86a7ad6134f7e9f1acde69fd7d861",
      "0c08aa77e51d4ce387309c1738f1e415",
      "4ab0658f76a6478292d3d41f33134440",
      "f71f0f6eb6374e0eb08465d38cb73a9c",
      "0c432019413049edaeed1b04cffb46d7",
      "e28896c8407f425b8a9d1d0c0cafbd40",
      "4330e804ce634b748f91f7d8457b74b9",
      "3785bd0e5ea44bd3979b99c2a9386fa3",
      "e2d5aa63bbb7498f90e571ba38e566b5",
      "49ef46d8216a4f8cbfff660c44b21610",
      "aa219d4dbcfb43879c85576a51dd236f",
      "574ab546c9fb483ca44b536b25d56861",
      "e3717a48e75043e29c57b5d0ff5f3380",
      "42c22daad9654b65bf6a391c92f03075",
      "ff6b9961af12456c87fc5b4a6cdd0061",
      "af02471317164e7eb3041ba4b5a545e2",
      "ec1d7e6cf8b645eca21486a5e25eb274",
      "b2ff33beb0274bbb86fc30b4074fd777",
      "b63a66db93394bd79b21e0091230a74c",
      "b008a71308ed4294b3886944664f22aa",
      "9322d90f1b6d47a29ee96e23f4fd96d6",
      "cd4d100d3f60458caf82f60ea50402b0",
      "14bc1beba60e44419305242575112b0b",
      "7157b349f13d4a5bb70ed2bb381ef417",
      "edef46651cd34f2783de8423a71e571b",
      "83f0c3653a0c4f7187ab433b654a491e",
      "b0f5d49219814c8e9c9a3edae2310a83",
      "334565bf2c57440da3784a84c75b154c",
      "cd27f8560ec643348457ba53dd4fad19",
      "d86975e7269b44ee8d83ac6990688e55",
      "20198400cb504e2db8d75e725d243b0c",
      "f6dcdf9e3c6c4784a23d88fc47e0f415",
      "c2071caab79f40e0b28ac0d1565fef43",
      "5d25ae60fcda4a4fbfedbb98cc5f4733",
      "3f6844ee1af74543b84da37506f3cb90",
      "4821a4f2951d41048a8f48658856b1df",
      "d4e9ef2666d046bea7a44bb479619997",
      "49f1f000f0424d9a9222e8e5ed548015",
      "2db14b9a96eb4ef5a7fbf1e8de067ed3"
     ]
    },
    "id": "yxCDu-GTrPSO",
    "outputId": "d2cf8fdd-bab6-4671-bcdf-f9bd4a0e5e18"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9776d06760584b37b8ef61ff1dfc21e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/7:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  train-loss 76.5539  |  Val Top-1 0.2324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8230e3abc9d84534aa514386868743ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/7:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  train-loss 6.4841  |  Val Top-1 0.2324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49e9793b2e64a3caaf2e6f5e9e24a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/7:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  train-loss 5.0935  |  Val Top-1 0.3418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190376ba45a047939802c5847e1a66a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/7:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  train-loss 4.4036  |  Val Top-1 0.3764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4330e804ce634b748f91f7d8457b74b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/7:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  train-loss 4.0078  |  Val Top-1 0.4040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ff33beb0274bbb86fc30b4074fd777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/7:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  train-loss 3.6889  |  Val Top-1 0.4142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd27f8560ec643348457ba53dd4fad19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/7:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  train-loss 3.3750  |  Val Top-1 0.4187\n"
     ]
    }
   ],
   "source": [
    "# ViLT fine-tuning with AMP and one-hot labels\n",
    "from tqdm.auto import tqdm\n",
    "from torch import amp\n",
    "\n",
    "\n",
    "EPOCHS  = 7                 #Number of fine tunning epoch\n",
    "scaler  = amp.GradScaler()\n",
    "model.train()               # model in training mode\n",
    "\n",
    "def val_top1(loader):\n",
    "    \"\"\"Hard Top-1 accuracy (expects one-hot labels).\"\"\"\n",
    "    model.eval(); correct = n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            labels = batch.pop(\"labels\")            #Extra hot one label\n",
    "            batch  = {k: v.to(DEVICE) for k,v in batch.items()}\n",
    "            logits = model(**batch).logits          # forward only\n",
    "            pred   = logits.argmax(1).cpu()         # predict class inex\n",
    "            gt     = labels.argmax(1)               # Ground truth class index\n",
    "            correct += (pred == gt).sum().item()    # couting the correct predictions\n",
    "            n      += pred.size(0)                  #switch back to training mode\n",
    "    model.train()\n",
    "    return correct / n\n",
    "\n",
    "# Training loop\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tot_loss = n_samples = 0\n",
    "\n",
    "    for batch in tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\"):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        with amp.autocast(device_type=\"cuda\"):\n",
    "            out  = model(**batch)       # BCEWithLogits loss via one-hot labels\n",
    "            loss = out.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Calculate total loss\n",
    "        tot_loss  += loss.item() * batch[\"labels\"].size(0)\n",
    "        n_samples += batch[\"labels\"].size(0)\n",
    "\n",
    "    #computing training loss and validation accuracy\n",
    "    train_loss = tot_loss / n_samples\n",
    "    val_acc    = val_top1(val_dl)\n",
    "    print(f\"Epoch {ep}:  train-loss {train_loss:.4f}  |  Val Top-1 {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165,
     "referenced_widgets": [
      "42a075d7da2f4eaca3a99016ac9f2872",
      "44cc889a885047fbae83562f2695c3ff",
      "ff6cbea7754b427696aae5482bd8b3b3",
      "0ef21574dcb34f8abbec49ad28e25352",
      "1632b9dfc52e4042879f1ad9a6a45638",
      "e74d8f32537042a59ba01880b4523669",
      "9e7ce2f1871b41ac8f585a0ca481d08c",
      "71771c28eedb483ebacce8b24f098153",
      "7a2d30f586f8460891196a8f5603aa7f",
      "63d8faa5e05741c3966f8fac212e7809",
      "c990ad66c66547cda3e1e0d166ebed78",
      "ae6fbe46f4c74ef883655f4466034ca6",
      "f64b3f16cbb64737b0749d99a0524171",
      "9931208f71cf4e7f8ab6c3cf1ff8b7a8",
      "5607d25e818c48299af8c9080ee1d2f4",
      "8c50acbacc33458383218293df21d2a9",
      "774af42ce67049fabb33678220ec330c",
      "6329e0f8f802425a9aba95553609a2a9",
      "80d4e46ed65e4545b9f9bcaa5752130b",
      "df2fe049f5864b96841720fdd716ccdd",
      "70d074d38d02407db447faa9b3dfc6cf",
      "9d43d502efac4b4bbacb9dd0ef20785f",
      "70b07b354f5a4708958e200fc2c23a4f",
      "84aa6de8b46f4810a853d78a65ba9d99",
      "eeb738d7f1ed46baab087d8589ad605b",
      "d00c1e59239749c298e168a36d431687",
      "044b3b0653b74039a44fdcdba4184180",
      "ae05139b8f4449499561e0c3f6ff465b",
      "20e0174ead8b4beea989721d54152ac1",
      "24d0b809071e4539ab80cf6afa056297",
      "a6a822a3f76842ca88fb5cc216e756d2",
      "d9bcfe64fb64424dab701554c70ff3cb",
      "3a2dd9c4553445a681ae8d98245a508f"
     ]
    },
    "id": "fYHHX5l054T0",
    "outputId": "cf3dfaaa-7fbe-4bc7-9358-ac44825b60c6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a075d7da2f4eaca3a99016ac9f2872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  train-loss 3.0520  |  Val Top-1 0.4342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6fbe46f4c74ef883655f4466034ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  train-loss 2.7312  |  Val Top-1 0.4360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b07b354f5a4708958e200fc2c23a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/1594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  train-loss 2.4199  |  Val Top-1 0.4413\n"
     ]
    }
   ],
   "source": [
    "# continue ViLT fine-tuning for epochs 8-10\n",
    "from tqdm.auto import tqdm\n",
    "from torch import amp\n",
    "\n",
    "EXTRA_EPOCHS = 3           # run 3 more epochs\n",
    "start_ep     = 8           # continue after the 7th epoch\n",
    "end_ep       = start_ep + EXTRA_EPOCHS - 1\n",
    "\n",
    "def val_top1(loader):\n",
    "    \"\"\"Hard Top-1 accuracy (expects one-hot labels).\"\"\"\n",
    "    model.eval(); correct = n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            labels = batch.pop(\"labels\")                 # (B,1001) one-hot\n",
    "            batch  = {k: v.to(DEVICE) for k,v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            pred   = logits.argmax(1).cpu()\n",
    "            gt     = labels.argmax(1)\n",
    "            correct += (pred == gt).sum().item()\n",
    "            n      += pred.size(0)\n",
    "    model.train()\n",
    "    return correct / n\n",
    "\n",
    "for ep in range(start_ep, end_ep + 1):\n",
    "    tot_loss = n_samples = 0\n",
    "    for batch in tqdm(train_dl, desc=f\"Epoch {ep}/{end_ep}\"):\n",
    "        batch = {k: v.to(DEVICE) for k,v in batch.items()}\n",
    "        with amp.autocast(device_type=\"cuda\"):\n",
    "            out  = model(**batch)            # BCEWithLogits loss\n",
    "            loss = out.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        tot_loss  += loss.item() * batch[\"labels\"].size(0)\n",
    "        n_samples += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_loss = tot_loss / n_samples\n",
    "    val_acc    = val_top1(val_dl)\n",
    "    print(f\"Epoch {ep}:  train-loss {train_loss:.4f}  |  Val Top-1 {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "8ac968be311e447ca29230c092ea0580",
      "ff6a0b056d394ad7b2fe008892c31603",
      "313bb5e6ad0a48f5b540ce11c8983b21",
      "ee9a8d18f9dd4eee83b97055a46599ee",
      "1ea87d00e69c492faa1ae18ca6da2900",
      "e7bcd6c3dbe54fc7906486cca0f8c8fc",
      "0277f9edf1c645279d8b4ea47884d73a",
      "6af74067c56148c4a3e9196ae962e3f8",
      "77460debfdd64528a7069ca83d823f99",
      "383b5bdd909e49ed8411f05e8f48ab86",
      "bd321486ae8e4b809557e9ef47cd587d"
     ]
    },
    "id": "Vs6P3u83rhBV",
    "outputId": "0cf84e61-5127-4d95-8d4e-9fc2bce8952a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac968be311e447ca29230c092ea0580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-1 accuracy : 0.4976\n",
      "VQA-soft acc  : 0.5303\n"
     ]
    }
   ],
   "source": [
    "# Computes Top-1 and VQA-soft accuracy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# helper functions\n",
    "def maj_answer(ans_list):\n",
    "    return Counter([a.lower() for a in ans_list]).most_common(1)[0][0]\n",
    "\n",
    "def vqa_soft_score(pred, ans_list):\n",
    "    cnt = sum(pred.lower() == a.lower() for a in ans_list)\n",
    "    return min(1.0, cnt / 3.0)\n",
    "\n",
    "# evaluation dataset\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, recs):\n",
    "        self.recs = recs\n",
    "        self.resize = transforms.Resize((224,224))\n",
    "    def __len__(self):  return len(self.recs)\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.recs[idx]\n",
    "        img = self.resize(Image.open(rec[\"image_path\"]).convert(\"RGB\"))\n",
    "        enc = processor(images=img,\n",
    "                        text   =rec[\"question\"],\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=MAX_Q_LEN,\n",
    "                        return_tensors=\"pt\")\n",
    "        sample = {k:v.squeeze(0) for k,v in enc.items()}\n",
    "        sample[\"answers\"] = rec[\"answers\"]   # keep full answer list\n",
    "        return sample\n",
    "\n",
    "def collate_eval(batch):\n",
    "    keys = [\"pixel_values\",\"input_ids\",\"attention_mask\"]\n",
    "    merged = {k: torch.stack([b[k] for b in batch]) for k in keys}\n",
    "    merged[\"answers\"] = [b[\"answers\"] for b in batch]\n",
    "    return merged\n",
    "\n",
    "eval_dl = DataLoader(EvalDataset(records), batch_size=32,\n",
    "                     shuffle=False, num_workers=2,\n",
    "                     collate_fn=collate_eval, pin_memory=True)\n",
    "\n",
    "# Dataset for evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval(); hard = soft = n = 0\n",
    "    for b in tqdm(loader, desc=\"Eval\"):\n",
    "        b_dev = {k:v.to(DEVICE) for k,v in b.items() if k!=\"answers\"}\n",
    "        logits = model(**b_dev).logits                     # (B, 1001)\n",
    "        preds  = [id2label[i] for i in logits.argmax(1).cpu().tolist()]\n",
    "        for p, ans_list in zip(preds, b[\"answers\"]):\n",
    "            hard += (p.lower() == maj_answer(ans_list))\n",
    "            soft += vqa_soft_score(p, ans_list)\n",
    "        n += len(preds)\n",
    "    return hard/n, soft/n\n",
    "\n",
    "top1, vqa = evaluate(eval_dl)\n",
    "print(f\"\\nTop-1 accuracy : {top1:.4f}\")\n",
    "print(f\"VQA-soft acc  : {vqa:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479,
     "referenced_widgets": [
      "48196b0bf95547b196631a070dbb1117",
      "3ee64e29c20840a8931f22d5a99bd6e1",
      "2ffd62ed70524ee780bcbf9bd18ac0b0",
      "e5eb329f899f4f3086d9a3a9475d2a1b",
      "dd702833d83247e7b28897cf5684ecb2",
      "bcb4edc5a4f24e5285cfff91a83afd9c",
      "31107e88e3c6459fb2ae32369f7741bc",
      "9997981904c14701b2527c58f1d144cf",
      "cf018a7f6ce2463e80838fe378ebf533",
      "ddb18e7ac1b746838ca1c8af3dda9a23",
      "4b4eb7bef38049d3b6c061afbd047acc"
     ]
    },
    "id": "ZcwOTa-g3wig",
    "outputId": "bf94ce89-ebfd-4ba8-cadf-3c92efeb0f16"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48196b0bf95547b196631a070dbb1117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict all:   0%|          | 0/15000 [00:10<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 question prefixes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2e982\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2e982_level0_col0\" class=\"col_heading level0 col0\" >qtype</th>\n",
       "      <th id=\"T_2e982_level0_col1\" class=\"col_heading level0 col1\" >acc%</th>\n",
       "      <th id=\"T_2e982_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2e982_level0_row0\" class=\"row_heading level0 row0\" >33</th>\n",
       "      <td id=\"T_2e982_row0_col0\" class=\"data row0 col0\" >could this</td>\n",
       "      <td id=\"T_2e982_row0_col1\" class=\"data row0 col1\" >100.0</td>\n",
       "      <td id=\"T_2e982_row0_col2\" class=\"data row0 col2\" >24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e982_level0_row1\" class=\"row_heading level0 row1\" >53</th>\n",
       "      <td id=\"T_2e982_row1_col0\" class=\"data row1 col0\" >do the</td>\n",
       "      <td id=\"T_2e982_row1_col1\" class=\"data row1 col1\" >95.2</td>\n",
       "      <td id=\"T_2e982_row1_col2\" class=\"data row1 col2\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e982_level0_row2\" class=\"row_heading level0 row2\" >43</th>\n",
       "      <td id=\"T_2e982_row2_col0\" class=\"data row2 col0\" >did the</td>\n",
       "      <td id=\"T_2e982_row2_col1\" class=\"data row2 col1\" >93.3</td>\n",
       "      <td id=\"T_2e982_row2_col2\" class=\"data row2 col2\" >30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e982_level0_row3\" class=\"row_heading level0 row3\" >9</th>\n",
       "      <td id=\"T_2e982_row3_col0\" class=\"data row3 col0\" >can you</td>\n",
       "      <td id=\"T_2e982_row3_col1\" class=\"data row3 col1\" >92.5</td>\n",
       "      <td id=\"T_2e982_row3_col2\" class=\"data row3 col2\" >67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2e982_level0_row4\" class=\"row_heading level0 row4\" >30</th>\n",
       "      <td id=\"T_2e982_row4_col0\" class=\"data row4 col0\" >is she</td>\n",
       "      <td id=\"T_2e982_row4_col1\" class=\"data row4 col1\" >92.0</td>\n",
       "      <td id=\"T_2e982_row4_col2\" class=\"data row4 col2\" >50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x79e7c61db6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bottom-5 question prefixes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cfe1b\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cfe1b_level0_col0\" class=\"col_heading level0 col0\" >qtype</th>\n",
       "      <th id=\"T_cfe1b_level0_col1\" class=\"col_heading level0 col1\" >acc%</th>\n",
       "      <th id=\"T_cfe1b_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cfe1b_level0_row0\" class=\"row_heading level0 row0\" >37</th>\n",
       "      <td id=\"T_cfe1b_row0_col0\" class=\"data row0 col0\" >where are</td>\n",
       "      <td id=\"T_cfe1b_row0_col1\" class=\"data row0 col1\" >3.1</td>\n",
       "      <td id=\"T_cfe1b_row0_col2\" class=\"data row0 col2\" >97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfe1b_level0_row1\" class=\"row_heading level0 row1\" >28</th>\n",
       "      <td id=\"T_cfe1b_row1_col0\" class=\"data row1 col0\" >what do</td>\n",
       "      <td id=\"T_cfe1b_row1_col1\" class=\"data row1 col1\" >0.0</td>\n",
       "      <td id=\"T_cfe1b_row1_col2\" class=\"data row1 col2\" >28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfe1b_level0_row2\" class=\"row_heading level0 row2\" >29</th>\n",
       "      <td id=\"T_cfe1b_row2_col0\" class=\"data row2 col0\" >where was</td>\n",
       "      <td id=\"T_cfe1b_row2_col1\" class=\"data row2 col1\" >0.0</td>\n",
       "      <td id=\"T_cfe1b_row2_col2\" class=\"data row2 col2\" >34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfe1b_level0_row3\" class=\"row_heading level0 row3\" >50</th>\n",
       "      <td id=\"T_cfe1b_row3_col0\" class=\"data row3 col0\" >what colors</td>\n",
       "      <td id=\"T_cfe1b_row3_col1\" class=\"data row3 col1\" >0.0</td>\n",
       "      <td id=\"T_cfe1b_row3_col2\" class=\"data row3 col2\" >29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cfe1b_level0_row4\" class=\"row_heading level0 row4\" >54</th>\n",
       "      <td id=\"T_cfe1b_row4_col0\" class=\"data row4 col0\" >what country</td>\n",
       "      <td id=\"T_cfe1b_row4_col1\" class=\"data row4 col1\" >0.0</td>\n",
       "      <td id=\"T_cfe1b_row4_col2\" class=\"data row4 col2\" >23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x79e7c60d8bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shows top & bottom question prefixes\n",
    "import re, pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Extracting first two words from question category\n",
    "def prefix(question):\n",
    "    tok = re.findall(r\"\\w+\", question.lower())\n",
    "    return \" \".join(tok[:2]) if len(tok) >= 2 else tok[0]\n",
    "\n",
    "# Predicting the answer for the single record using ViLT model\n",
    "@torch.no_grad()\n",
    "def vilt_predict(rec):\n",
    "    img = transforms.Resize((224,224))(Image.open(rec[\"image_path\"]).convert(\"RGB\"))\n",
    "    enc = processor(images=img, text=rec[\"question\"], return_tensors=\"pt\").to(DEVICE)\n",
    "    idx = model(**enc).logits.argmax(1).item()\n",
    "    return id2label[idx]\n",
    "\n",
    "# gather stats\n",
    "stats = defaultdict(lambda:[0,0])        # correct, total\n",
    "for rec in tqdm(records, desc=\"Predict all\"):\n",
    "    pred = vilt_predict(rec)\n",
    "    gt   = maj_answer(rec[\"answers\"])\n",
    "    pfx  = prefix(rec[\"question\"])\n",
    "    stats[pfx][1] += 1\n",
    "    if pred.lower() == gt.lower():\n",
    "        stats[pfx][0] += 1\n",
    "\n",
    "rows = [{\"qtype\": k, \"acc%\": 100*c/t, \"count\": t}\n",
    "        for k,(c,t) in stats.items() if t >= 20]          # ignore rare types\n",
    "df = pd.DataFrame(rows).sort_values(\"acc%\", ascending=False)\n",
    "\n",
    "print(\"Top-5 question prefixes\")\n",
    "display(df.head(5).style.format({\"acc%\":\"{:.1f}\"}))\n",
    "print(\"\\nBottom-5 question prefixes\")\n",
    "display(df.tail(5).style.format({\"acc%\":\"{:.1f}\"}))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
